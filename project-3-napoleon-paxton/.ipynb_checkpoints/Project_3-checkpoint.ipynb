{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding User Behavior in a Video Game\n",
    "By: Napoleon Paxton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The goal of this document is to provide details on how to track events on a video game. This document provides details of the generated data, data pipeline, and example analytics of the data. The hope is that this document can be used as a template for engineers that would like to add events or conduct analytics for Delta Quest. This document can also be used as a guide for engineers that would like to observe the current pipeline and create updates based off of it. For engineers that are only interested in the commands used to create the pipeline and example analysis in this document without verbose context, see Appendix 1. This document requires the use of multiple command terminals to operate. For ease of use, Appendix 1 is separated by the command terminal used to enter the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. Connecting Jupyter Notebook with PySpark\n",
    "2. Create Kafka Topic and Flask Web Server\n",
    "3. Watch Events with Kafkacat\n",
    "4. Generate Events with Apache Bench\n",
    "5. Filter Schema\n",
    "6. View Purchaces in Hadoop\n",
    "7. Read Pyspark for queries in code cell\n",
    "8. Query with Hive\n",
    "9. Query with Presto\n",
    "10. Streaming (filter swords)\n",
    "11. Streaming (write swords)\n",
    "12. Streaming (continuously feed the stream)\n",
    "13. Check data populated in Hadoop\n",
    "14. Appendix 1: List of Commands by Terminal Shells Opened\n",
    "15. Appendix 2: Event generating file\n",
    "16. Appendix 3: Redis storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connecting Jupyter Notebook with PySpark\n",
    "For my environment I am using a Unix shell on Google Cloud Platform (GCP). The first thing I do is to spin up all the containers I need which are contained in the docker-compose.yml file using the following command: <b>docker-compose up -d</b>. Next, within the same bash shell I create a symbolic link so I can connect a jupyter notebook with pyspark for analysis. First I enter the spark bash shell like this: <b>docker-compose exec spark bash</b>, and then using this command: <b>ln -s /w205 w205</b>, I generate the symbolic link. Once this link is created you can then exit the container simply by typing <b>exit</b>. Finally, within this same shell, I use this command to start a Jupyter Notebook for a pyspark kernel: <b>docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark</b>. Once the url is generated, copy the link into a text editor and then replace the address octets <b>(--ip 0.0.0.0)</b> with the correct values representing the location of your computing environment. \n",
    "## 2. Create Kafka Topic and Flask Web Server\n",
    "Open up another shell in your computing environment and run this command to create a kafka topic: <b>docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181</b>. Here we called the topic events, so you know you succeeded when you see the message: <b> Created topic events </b>. Next we spin up our web application game using Flask: <b> docker-compose exec mids env FLASK_APP=/w205/full-stack2/game_api.py flask run --host 0.0.0.0 </b>. You know you have succeeded when you see the messages: <b> * Serving Flask app \"game_api\" and * Running on http://0.0.0.0:5000/ </b>\n",
    "\n",
    "## 3. Watch Events with Kafkacat\n",
    "Next we want to open another shell to start kafkacat so we can watch the events on kafka: <b> docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning </b>. \n",
    "\n",
    "## 4. Generate Events with Apache Bench\n",
    "Next we generate events using Apache bench. \n",
    "\n",
    "    * docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/\n",
    "    * docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword\n",
    "    * docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_staff\n",
    "    * docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/\n",
    "    * docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_sword\n",
    "    * docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_staff\n",
    "After running the command you will see GET requests on the Flask web application and you will see the events populated in kafkacat. \n",
    "\n",
    "## 5. Filter Schema\n",
    "Next we can use the following code to filter out data and write to hdfs: <b> docker-compose exec spark spark-submit /w205/full-stack/filtered_writes.py</b>. \n",
    "\n",
    "## 6. View Purchases in Hadoop\n",
    "We can then see these purchases in hdfs using the following commands: <b> (1) docker-compose exec cloudera hadoop fs -ls /tmp/ </b> <b> (2) docker-compose exec cloudera hadoop fs -ls /tmp/purchases/</b>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Read Pyspark for queries in code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-83b7e2a213b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Add purchases which are currently in hadoop to a variable and show the purchases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/purchases'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "## Add purchases which are currently in hadoop to a variable and show the purchases\n",
    "df = spark.read.parquet('/tmp/purchases')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|    user2.att.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|Accept|             Host|     User-Agent|    event_type|           timestamp|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|2021-08-06 03:11:...|\n",
      "+------+-----------------+---------------+--------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add purchases which are currently in hadoop to a variable\n",
    "df = spark.read.parquet('/tmp/purchases')\n",
    "\n",
    "## Show those purchases\n",
    "df.show()\n",
    "\n",
    "## Create a temporary table named purchases based on the data in the variable\n",
    "df.registerTempTable('Purchases')\n",
    "\n",
    "## Select everything from purchases from the host user1.comcast.com and show them\n",
    "df_by_example2 = spark.sql(\"select * from purchases where host='user1.comcast.com'\")\n",
    "df_by_example2.show()\n",
    "\n",
    "## Create a Pandas dataframe based on df_by_example2 variable and show details of the dataframe\n",
    "newdf = df_by_example2.toPandas()\n",
    "newdf.describe()\n",
    "\n",
    "## Store a sql query in a varaible named query and then run a spark sql query command using that variable\n",
    "query = \"create external table purchase_events stored as parquet location '/tmp/purchase_events' as select * from purchases\"\n",
    "spark.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Query with Hive\n",
    "Write a hive table to hdfs using this command: <b> docker-compose exec spark spark-submit /w205/full-stack2/write_hive_table.py </b>\n",
    "Now we can check to see if it is listed in hdfs using these commands: \n",
    "<b> (1) docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "(2) docker-compose exec cloudera hadoop fs -ls /tmp/purchases/ </b>\n",
    "\n",
    "## 9. Query with Presto\n",
    "Now we can use Presto to query hdfs. Once we enter this command: <b> docker-compose exec presto presto --server presto:8080 --catalog hive --schema default </b>, we are now in Presto and can make queries. Some example commands are <b> show tables, describe purchases, and select * from purchases </b>\n",
    "\n",
    "## 10. Streaming (filter swords)\n",
    "We can also stream events. The following file filters schemas in a streaming format: <b> docker-compose exec spark spark-submit /w205/full-stack2/filter_swords_stream.py </b> \n",
    "\n",
    "## 11. Streaming (write swords)\n",
    "We can also write a stream to parquet format using the following file: <b> docker-compose exec spark spark-submit /w205/full-stack2/write_swords_stream.py </b> \n",
    "\n",
    "## 12. Streaming (continuously feed the stream)\n",
    "Next, here is an example of how we can continuously feed the stream: <b> while true; do docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword; sleep 5; done </b>. \n",
    "\n",
    "## 13. Check Data Populated in Hadoop\n",
    "To see what is populated in hadoop, run this command: <b> docker-compose exec cloudera hadoop fs -ls /tmp/sword_purchases </b>. Since we are now streaming, wait a few minutes and run this command again so you can see how it has changed over time. Once this is done, make sure to use the command <b> docker-compose down </b> to shut down your containers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Appendix 1: Commands based on the Terminal Shells Opened\n",
    "### Commands in Shell 1\n",
    "1. Start up containers\n",
    "    * docker-compose up -d\n",
    "2. Enter Spark bash to create symbolic link for Juypter Notebook\n",
    "    * docker-compose exec spark bash\n",
    "3. Create symbolic link\n",
    "    * ln -s /w205 w205\n",
    "4. Leave Spark bash\n",
    "    * exit\n",
    "5. Open up Spark based Jupyter Notebook\n",
    "    * docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "    \n",
    "### Commands in Shell 2    \n",
    "1. Create Kafka topic\n",
    "    * docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181 \n",
    "2. Start Flask Web Application\n",
    "    * docker-compose exec mids env FLASK_APP=/w205/full-stack2/game_api.py flask run --host 0.0.0.0\n",
    "\n",
    "### Commands in Shell 3\n",
    "1. Start Kafkacat for monitoring\n",
    "    * docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning\n",
    "\n",
    "### Commands in Shell 4\n",
    "1. Generate Events with Apache Bench \n",
    "    * docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/\n",
    "    * docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword\n",
    "    * docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/\n",
    "    * docker-compose exec mids ab -n 10 -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_sword\n",
    "2. Filter Schema\n",
    "    * spark spark-submit /w205/full-stack2/filtered_writes.py\n",
    "3. View purchaces in Hadoop\n",
    "    * docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "    * docker-compose exec cloudera hadoop fs -ls /tmp/purchases/\n",
    "4. Query with Hive\n",
    "    * docker-compose exec spark spark-submit /w205/full-stack2/write_hive_table.py\n",
    "5. Query with Presto\n",
    "    * docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "    * docker-compose exec cloudera hadoop fs -ls /tmp/purchases/\n",
    "\n",
    "### Commands in Shell 5\n",
    "1. Filter Stream\n",
    "    * docker-compose exec spark spark-submit /w205/full-stack2/filter_swords_stream.py\n",
    "\n",
    "### Commands in Shell 6\n",
    "1. Write HDFS Files in Streaming Mode\n",
    "    * docker-compose exec spark spark-submit /w205/full-stack2/write_swords_stream.py\n",
    "\n",
    "### Commands in Shell 7\n",
    "1. Continuously Feed Stream\n",
    "    * while true; do docker-compose exec mids ab -n 10 -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword; sleep 5; done\n",
    "\n",
    "### Commands in Shell 8\n",
    "1. Check hadoop\n",
    "    * docker-compose exec cloudera hadoop fs -ls /tmp/sword_purchases\n",
    "2. Shutdown Containers\n",
    "    * docker-compose down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 15. Appendix 2: Event Generating File\n",
    "Included in this repo is an event generating shell script file which when run will push events to Kafka. To run the file use the following command: <b> ./data_generation.txt </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Appendix 3: Redis Storage\n",
    "To add Redis for storage we create a new image. Using redis will allow you to do things like track the state of users. We use the following commands to start:\n",
    "    * FROM midsw205/base\n",
    "    * MAINTAINER Your Name <youremail>\n",
    "    * RUN pip install redis\n",
    "Or we can build a redis image:\n",
    "    * docker build -t midswredis\n",
    "Next we modify the docker-compose.yml file. We add the redis entry and then we change mids to use the redis image:\n",
    "    * redis:\n",
    "    *     image: redis:latest\n",
    "    *     expose:\n",
    "    *        - \"6379\"\n",
    "    *     ports:\n",
    "    *        - \"6379:6379\"\n",
    "    * mids:\n",
    "    *     image: midswredis\n",
    "    *     stdin_open: true\n",
    "    *     tty: true\n",
    "    *     volumes:\n",
    "    *       - /Your/Path/w205:/w205\n",
    "    *     expose:\n",
    "    *       - \"5000\"\n",
    "    *     ports:\n",
    "    *       - \"5000:5000\"\n",
    "    *     extra_hosts:\n",
    "    *       - \"moby:127.0.0.1\"\n",
    "    *\n",
    "    \n",
    "To use, we enter the command: <b> docker-compose exec mids bash </b>. This logs us into the new redis image. Next we enter Python by typing <b> ipython </b> and we enter the following command to use redis <b> import redis </b>. We then connect to the local service using <b> r = redis.Redis(host='redis', port='6379') </b>. Some example things you can do is check the keys using <b> r.keys(). If you don't have something running the keys will be empty. Next you can set keys using the command <b> r.mset({\"Croatia\": \"Zagreb\", \"Bahams\": \"Nassau\"}) </b>. When you check the keys again using <b> r.keys() </b> you will see Zagreb, Bahams, and Nassau entered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
